{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c725cd2",
   "metadata": {},
   "source": [
    "# ADS-509 Assignment 1.1\n",
    "## Collecting Text Data via API and Web Scraping\n",
    "\n",
    "**Student Version**  \n",
    "\n",
    "In this assignment you will:\n",
    "- Collect story metadata via an API (Hacker News).\n",
    "- Scrape discussion comments from HTML using BeautifulSoup.\n",
    "- Merge into one tidy dataset (one row per comment with story metadata).\n",
    "- Save results and run some quick checks/EDA.\n",
    "\n",
    "Although the discussion posts are available via the HackerNews API, this mirrors real-world pipelines where structured APIs don’t expose the exact text you need, so you complement them with carefully designed scrapers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d8b21",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
    "\n",
    "Work through this notebook as if it were a worksheet, completing the code sections marked with **TODO** in the cells provided. Similarly, written questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you to fill in with your answers. **Make sure to answer every question marked with a Q: for full credit**.\n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential import statements and make sure that all such statements are moved into the designated cell.\n",
    "\n",
    "A .pdf of this notebook, with your completed code and written answers, is what you should submit in Canvas for full credit. **DO NOT SUBMIT A NEW NOTEBOOK FILE OR A RAW .PY FILE**. Submitting in a different format makes it difficult to grade your work, and students who have done this in the past inevitably miss some of the required work or written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe3fac",
   "metadata": {},
   "source": [
    "## Imports and Definitions\n",
    "\n",
    "First we import our libraries, set up our folder structure, and define some useful functions and variables.\n",
    "\n",
    "**TODO**\n",
    "- update the directory names with whatever path you would like to use to store your data\n",
    "- define a function that will cause your code to \"sleep\" for a random interval between 1 and 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# add any other import statements here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6709f-f16a-4f6c-9184-9ab29e602886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polite scraping: set a clear user agent and throttle requests a bit\n",
    "HEADERS = {\"User-Agent\": \"MADS-LLM-2025-Student/1.0 (+https://example.edu)\"}\n",
    "BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "def sleep_politely(sleep_range=(1.0,2.0)):\n",
    "    # TODO: define a random sleep function\n",
    "    ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a9710-e675-488b-938a-69fd0c3a49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your folder structure\n",
    "# TODO: Update the directory names as needed\n",
    "DATA_DIR = 'data/module1'\n",
    "RAW_API_DIR = os.path.join(DATA_DIR, 'raw_api')\n",
    "RAW_HTML_DIR = os.path.join(DATA_DIR, 'raw_html')\n",
    "for d in [DATA_DIR, RAW_API_DIR, RAW_HTML_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19b125-b58f-4909-89eb-5f9a95702906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add any other helper functions that you define here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b5745",
   "metadata": {},
   "source": [
    "## API Data Collection\n",
    "\n",
    "Here we will use the **requests** library to interact with the HackerNews API. Our ultimate goal is to create a dataset with metadata and discussion posts for 50 of the top stories on this website, so first we will use the API to get the top story IDs.\n",
    "\n",
    "**TODO**:\n",
    "- Read through the HackerNews API documentation (https://github.com/HackerNews/API) and use the requests library to pull the IDs for all top news stories. *Be sure to use the headers and timeout arguments*.\n",
    "\n",
    "**Q:** What do the headers and timeout arguments in the requests library do?\n",
    "\n",
    "**A:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a big list of top story IDs\n",
    "r = ?? # TODO: Use the requests library to pull the IDs for the top stories\n",
    "r.raise_for_status()\n",
    "top_ids = r.json()\n",
    "print('Total top story ids returned:', len(top_ids))\n",
    "\n",
    "# For a faster assignment, we’ll just use the first 50 stories\n",
    "CANDIDATE_N = 50\n",
    "candidate_ids = top_ids[:CANDIDATE_N]\n",
    "len(candidate_ids), candidate_ids[:5] # Print 5 of the IDs as a self-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4b774",
   "metadata": {},
   "source": [
    "Next we need to use our list of article IDs to pull and organize the metadata that we want to include in our final dataset.\n",
    "\n",
    "**TODO**:\n",
    "- Use the HackerNews API documentation to pull the metadata for each of the stories in your candidate_ids list.\n",
    "- Filter your stories for those with discussion threads. Hint: look for the \"descendants\" column.\n",
    "- Format your stories with their associated metadata into a pandas dataframe. Hint: check out the Pandas `json_normalize()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_item(item_id):\n",
    "    # cache to disk so re-runs are fast and reproducible\n",
    "    fp = os.path.join(RAW_API_DIR, f\"item_{item_id}.json\")\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    r = ?? # TODO: Use the requests library to pull the metadata for a given story ID\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    with open(fp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    sleep_politely()\n",
    "    return data\n",
    "\n",
    "items = []\n",
    "for idx, i in enumerate(candidate_ids):\n",
    "    if idx%5==0:\n",
    "        print(f\"{idx}/{len(candidate_ids)}\")\n",
    "    items.append(fetch_item(i))\n",
    "stories = # TODO: filter your data for stories with comment threads\n",
    "\n",
    "#TODO: Format your data into a pandas dataframe with one row per story, keeping only the metadata columns listed in keep_cols\n",
    "keep_cols = [\"id\", \"title\", \"by\", \"time\", \"url\", \"score\", \"descendants\"]\n",
    "stories_df = ??\n",
    "\n",
    "print('Stories w/ discussion:', len(stories_df))\n",
    "stories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359f73c",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "Next, we will use our list of story IDs to scrape the discussion threads from HackerNews using the BeautifulSoup library.\n",
    "\n",
    "**TODO**:\n",
    "- Use the requests library to pull the html for each story's comment page on Hacker News (https://news.ycombinator.com/). Hint: you'll need to go to the website to see how to format your url properly.\n",
    "- Use the BeautifulSoup library to extract the user, comment id, time span, and comment text from each story's HTML. Hint: use the html inspector (ctrl+shift+i) to identify relevant html tags in your browser.\n",
    "- Format the comment threads into a pandas dataframe with the story id and one row per comment.\n",
    "\n",
    "**Q**: Find the HackerNews robots.txt. What does it say about scraping, and are we acting within its stated policy?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discussion_html(story_id):\n",
    "    fp = os.path.join(RAW_HTML_DIR, f\"hn_{story_id}.html\")\n",
    "    if os.path.exists(fp):\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    url = ?? # TODO: use the requests library to pull the html for each story's comment page\n",
    "    r = ??\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "    with open(fp, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    sleep_politely(sleep_range = (30.0,31.0)) # Adjust sleep according to robots.txt\n",
    "    return html\n",
    "\n",
    "# Simple HTML -> comments parser\n",
    "def parse_comments_from_html(html, story_id):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    rows = []\n",
    "    # TODO: Extract comment_id, user, time span, and comment text from each comment in the html\n",
    "    for tr in ?? :\n",
    "        comment_id = ??\n",
    "        user = ??\n",
    "        time_text = ??\n",
    "        comment_text = ??\n",
    "        if comment_text:\n",
    "            rows.append({\n",
    "                'story_id': story_id,\n",
    "                'comment_id': comment_id,\n",
    "                'user': user,\n",
    "                'time_text': time_text,\n",
    "                'comment_text': comment_text,\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "all_comments = []\n",
    "for idx, sid in enumerate(stories_df['id'].tolist()):\n",
    "    if idx%5==0:\n",
    "        print(f\"{idx}/{len(stories_df['id'])}\")\n",
    "    html = get_discussion_html(sid)\n",
    "    all_comments.extend(parse_comments_from_html(html, sid))\n",
    "\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "print('Total comments scraped:', len(comments_df))\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d947e",
   "metadata": {},
   "source": [
    "## Combine Datasets\n",
    "\n",
    "Now we have two dataframes that need to be combined into a single dataset. Luckily, these dataframes have a shared key which will make it relatively simple to combine them, but that is often not the case, so you need to be creative about which pieces of data you could use to merge data from multiple sources.\n",
    "\n",
    "**TODO**\n",
    "- Combine your `stories_df` and `comments_df` dataframes using the shared key so that the resulting dataset has one line per comment.\n",
    "- As a final cleaning step, convert the timestamp column from Unix epoch format to a pandas datetime format\n",
    "\n",
    "**Q**: If we didn't have a shared key for the two dataframes in this scenario, what could you use instead to join them?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge your two dataframes\n",
    "merged_df = ??\n",
    "\n",
    "# TODO: convert the timestamp to datetime format\n",
    "merged_df['story_time'] = ??\n",
    "\n",
    "assert merged_df['comment_text'].notna().all()\n",
    "print('Rows in merged dataset:', len(merged_df))\n",
    "merged_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bd018",
   "metadata": {},
   "source": [
    "## Save your dataset for future use\n",
    "\n",
    "We will be using this dataset in future assignments, so use this code to save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ead8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = os.path.join(DATA_DIR, 'hn_comments_with_storymeta.csv')\n",
    "merged_df.to_csv(out_csv, index=False)\n",
    "out_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f84664",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "This section is used for grading. Please run these cells before submission, but do not change any of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_per_story = (\n",
    "    merged_df.groupby(['story_id','title']).size()\n",
    "    .rename('n_comments').reset_index()\n",
    ")\n",
    "comments_per_story.sort_values('n_comments', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    comments_per_story.sort_values('n_comments')\n",
    "    .tail(15)\n",
    "    .plot(kind='barh', x='title', y='n_comments', figsize=(8,6))\n",
    ")\n",
    "ax.set_xlabel('Number of comments')\n",
    "ax.set_ylabel('Story title')\n",
    "ax.set_title('Most discussed HN stories (sample)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9637fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['user'].fillna('unknown').value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe40a4-5eb4-4e1d-b582-2cfca079bfc9",
   "metadata": {},
   "source": [
    "<small>This assignment was designed with the assistance of ChatGPT.</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
