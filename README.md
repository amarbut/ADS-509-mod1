# Collecting Text Data via API and Web Scraping
Acquiring data is one of the fundamental steps in any analysis, and proficiency at APIs and web scraping unlocks rich data sets for future analyses. In this assignment you will:
- Collect story metadata via an API (Hacker News).
- Scrape discussion comments from HTML using BeautifulSoup.
- Merge into one tidy dataset (one row per comment with story metadata).
- Save results and run some quick checks/EDA.

## Instructions

1. Create a repository under your GitHub account from this template: [https://github.com/amarbut/ADS-509-mod1](https://github.com/amarbut/ADS-509-mod1). Instructions can be found [here](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template). Make your repository [private](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/managing-repository-settings/setting-repository-visibility) and add your instructorâ€™s GitHub account as a [collaborator](https://docs.github.com/en/account-and-profile/how-tos/setting-up-and-managing-your-personal-account-on-github/managing-access-to-your-personal-repositories/inviting-collaborators-to-a-personal-repository). 
2. Follow the provided Jupyter Notebook like a worksheet, completing all of the **TODO** items in the provided code cells and answering all of the written questions (marked **Q**) in the provided markdown cells. DO NOT create your own notebook or .py file from scratch, as this inevitably leads to missed instructions or questions and complicates grading.
3. When you have finished your code, print your notebook as a PDF and submit it in Canvas. 
4. Commit your code, push the changes to GitHub, and include a link to your repo in your Canvas submission note so your instructor has access to the .ipynb notebook files and any other code you create.

